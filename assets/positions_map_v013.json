{"nodes": [{"data": {"id": "Probability_&_Statistics", "name": "Probability & Statistics", "colour": "#ff00ff", "nodetype": "field"}, "position": {"x": 2951.6893965031513, "y": 2906.486086513617}}, {"data": {"id": "Linear_Algebra", "name": "Linear Algebra", "colour": "#ff7eff", "nodetype": "field"}, "position": {"x": 1117.9566212171549, "y": 2776.964604678666}}, {"data": {"id": "Reinforcement_Learning", "name": "Reinforcement Learning", "colour": "#d4ddff", "nodetype": "field"}, "position": {"x": 1783.9530342094786, "y": 790.093400267294}}, {"data": {"id": "Deep_Learning", "name": "Deep Learning", "colour": "#7effff", "nodetype": "field"}, "position": {"x": 3927.798134239869, "y": 15.50380357787941}}, {"data": {"id": "Calculus", "name": "Calculus", "colour": "#2addff", "nodetype": "field"}, "position": {"x": 4396.159921625797, "y": 2996.3690148600813}}, {"data": {"id": "Optimization", "name": "Optimization", "colour": "#2c7eff", "nodetype": "field"}, "position": {"x": 4539.54557214549, "y": 1317.5589658084475}}, {"data": {"id": "Machine_Learning", "name": "Machine Learning", "colour": "#8000ff", "nodetype": "field"}, "position": {"x": 3244.691194331005, "y": 1114.0411597924879}}, {"data": {"id": "1", "name": "What is a Matrix?", "lectures": "Linear Algebra", "description": "A matrix is essentially a container of scalar numbers.", "urls": ["https://www.khanacademy.org/math/preCalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:mat-intro/v/introduction-to-the-matrix", "https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-adding-and-subtracting-matrices/v/matrix-addition-and-subtraction-1", "https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-multiplying-matrices-by-scalars/v/scalar-multiplication"], "nodetype": "concept", "relative_importance": 1.5652475842498528, "parent": "Linear_Algebra"}, "position": {"x": 1301.164765611563, "y": 3595.0445170881317}}, {"data": {"id": "2", "name": "Matrix Multiplication", "lectures": "Linear Algebra", "description": "Calculate the product of two matrices", "urls": ["https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-multiplying-matrices-by-matrices/v/matrix-multiplication-intro", "https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-properties-of-matrix-multiplication/v/defined-and-undefined-matrix-operations", "https://www.youtube.com/watch?v=XkY2DOUCWMU"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Linear_Algebra"}, "position": {"x": 1300.2254389913276, "y": 3012.4028035529827}}, {"data": {"id": "3", "name": "Inversion & Singular Matrices", "lectures": "Linear Algebra", "description": "Inversion is the process of find the matrix that multiplies a matrix to produce the identity matrix. Singular matrices are non-invertible.", "urls": ["https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-intro-to-matrix-inverses/v/inverse-matrix-part-1", "https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-finding-inverse-matrix-with-determinant/v/inverse-of-a-2x2-matrix", "https://www.youtube.com/watch?v=KBYvP6YG58g"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Linear_Algebra"}, "position": {"x": 835.4553728877601, "y": 2712.860192347071}}, {"data": {"id": "4", "name": "Linear Equations (Ax=b)", "lectures": "Linear Algebra", "description": "Using linear algebra to solve linear equations", "urls": ["https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-solving-equations-with-inverse-matrices/v/matrix-equations-systems", "https://www.coursera.org/lecture/linear-algebra-machine-learning/matrices-vectors-and-solving-simultaneous-equation-problems-jGab3"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Linear_Algebra"}, "position": {"x": 1387.185213439191, "y": 2292.7283606190454}}, {"data": {"id": "5", "name": "Vector Spaces & Basis", "lectures": "Linear Algebra", "description": "A vector space is a set of vectors in which any scalar multiplication can occur and the result remains in the space", "urls": ["https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/subspace-basis/v/linear-subspaces", "https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/subspace-basis/v/linear-algebra-basis-of-a-subspace", "https://www.youtube.com/watch?v=XDvSsDsLVLs"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Linear_Algebra"}, "position": {"x": 829.9261294044602, "y": 2368.7286331157175}}, {"data": {"id": "7", "name": "Permutations", "lectures": "Linear Algebra", "description": "", "urls": ["https://www.youtube.com/watch?v=d7AovBKeNMI", "https://www.youtube.com/watch?v=8OSAsm5tTwU", "https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-5-transposes-permutations-spaces-r-n"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 366.7695301964734, "y": 1790.496794403749}}, {"data": {"id": "9", "name": "Random Variables", "lectures": "Probability & Statistics", "description": "Variables whose possible values are numerical outcomes of a random phenomenon", "urls": ["https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library", "https://www.youtube.com/watch?v=vfqPpai_9jI"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 2926.4501967582823, "y": 3779.732427268573}}, {"data": {"id": "10", "name": "Probability Axioms", "lectures": "Probability & Statistics", "description": "What kind of events can we express with probabilities? The axioms of probability are the fundamental rules that define probabilities.", "urls": ["https://www.youtube.com/watch?v=UjcGyISekpE", "https://www.youtube.com/watch?v=pA83XtLeVig", "https://www.youtube.com/watch?v=xuv6BCR-iNc"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 2925.9770467974827, "y": 3995.8302934677827}}, {"data": {"id": "11", "name": "Probability density/mass functions", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/probability-density-functions", "https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/probabilities-from-density-curves", "https://www.youtube.com/watch?v=iThbNOsDj0Q", "https://www.youtube.com/watch?v=YXLVjCKVP7U"], "nodetype": "concept", "relative_importance": 1.9798989873223332, "parent": "Probability_&_Statistics"}, "position": {"x": 2925.09524624152, "y": 3543.936150384139}}, {"data": {"id": "12", "name": "Cumulative Distribution Functions", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.youtube.com/watch?v=YXLVjCKVP7U", "https://www.youtube.com/watch?v=ZJsOOCghQJ0"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3559.6004320064435, "y": 3267.8202735676978}}, {"data": {"id": "13", "name": "Expectations", "lectures": "Probability & Statistics", "description": "What you would expect the value of random variables to be on average", "urls": ["https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals", "https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/expected-value-of-a-discrete-random-variable", "https://www.youtube.com/watch?v=9VbqyziBjrg"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3770.94852618318, "y": 2851.1798080953868}}, {"data": {"id": "14", "name": "Conditional Probability & Independence", "lectures": "Probability & Statistics", "description": "The rules governing different random variables that are correlated or depend on one another", "urls": ["https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals", "https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3338.538209102102, "y": 2720.9708669190723}}, {"data": {"id": "15", "name": "Variance", "lectures": "Probability & Statistics", "description": "Variance is a measure of the spread of data from their mean value", "urls": ["https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Probability_&_Statistics"}, "position": {"x": 2865.0340643366235, "y": 2679.513127834526}}, {"data": {"id": "16", "name": "Bayes' Theorem", "lectures": "Probability & Statistics", "description": "Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event.", "urls": ["https://www.youtube.com/watch?v=HZGCoVF3YvM", "http://www.mas.ncl.ac.uk/~nlf8/teaching/mas2317/notes/chapter2.pdf", "http://pillowlab.princeton.edu/teaching/mathtools16/slides/lec13_BayesRule.pdf", "https://seeing-theory.brown.edu/#secondPage/chapter5"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Probability_&_Statistics"}, "position": {"x": 3347.927482909129, "y": 2069.860602805957}}, {"data": {"id": "17", "name": "Differentiation", "lectures": "Calculus", "description": "Differentation is the process of finding the differential of a function, which is a function describing the sensitivity to change of it's argument", "urls": ["https://www.khanacademy.org/math/differential-Calculus/dc-diff-intro/dc-derivative-intro/v/Calculus-derivatives-1-new-hd-version?modal=1", "https://www.youtube.com/watch?v=9AI3BkKQhn0", "https://www.youtube.com/watch?v=NRSmIE5MMBQ&list=PL5KkMZvBpo5DwIsDKWdHYmkRZmXMi1mE8", "https://www.youtube.com/watch?v=lowavG2SXsQ&list=PLmdFyQYShrjd4Qn42rcBeFvF6Qs-b6e-L&index=9", "https://www.youtube.com/watch?v=AOkn9-UK5AU&list=PLmdFyQYShrjd4Qn42rcBeFvF6Qs-b6e-L&index=10", "https://www.derivative-calculator.net/"], "nodetype": "concept", "relative_importance": 1.7146428199482244, "parent": "Calculus"}, "position": {"x": 4326.690468337576, "y": 3974.735038935957}}, {"data": {"id": "18", "name": "Product Rule", "lectures": "Calculus", "description": "A method for finding the derivatives of the products of two or more functions", "urls": ["https://www.youtube.com/watch?v=17X5g9QArTc", "https://www.youtube.com/watch?v=h78GdGiRmpM"], "nodetype": "concept", "relative_importance": 1, "parent": "Calculus"}, "position": {"x": 4783.671347857931, "y": 3254.7282276569617}}, {"data": {"id": "19", "name": "Chain Rule", "lectures": "Calculus", "description": "The chain rule provides a method of finding the derivative of composite functions. Knowing the change in z relative y and change in y relative to x lets us find the rate of change of z relative to x.", "urls": ["https://www.khanacademy.org/math/ap-Calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction"], "nodetype": "concept", "relative_importance": 1, "parent": "Calculus"}, "position": {"x": 4802.503693995598, "y": 2069.4370094690394}}, {"data": {"id": "20", "name": "Integration", "lectures": "Calculus", "description": "Methods for finding the area under a curve of a function", "urls": ["https://www.khanacademy.org/math/ap-Calculus-ab/ab-integration-new/ab-6-1/v/introduction-to-integral-Calculus"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Calculus"}, "position": {"x": 4005.8843049423085, "y": 3683.2538154937392}}, {"data": {"id": "22", "name": "Cost functions", "lectures": "Optimization", "description": "Measures of how wrong a model is in estimating the relationship between it's inputs and the desired output", "urls": ["https://www.youtube.com/watch?v=euhATa4wgzo", "https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220"], "nodetype": "concept", "relative_importance": 1, "parent": "Optimization"}, "position": {"x": 4585.383796604388, "y": 1564.479992078418}}, {"data": {"id": "23", "name": "Intro to Optimisation", "lectures": "Optimization", "description": "The process of maximising or minimizing a function by systematically choosing input values", "urls": ["https://www.khanacademy.org/math/ap-calculus-ab/ab-diff-analytical-applications-new/ab-5-11/v/minimizing-sum-of-squares", "http://scipy-lectures.org/advanced/mathematical_optimization/", "https://deepai.org/machine-learning-glossary-and-terms/mathematical-optimization", "https://www.youtube.com/watch?v=vwUV2IDLP8Q", "https://www.youtube.com/watch?v=Ef22yTJDUZI", "https://www.youtube.com/watch?v=HsUY94Fjxao", "https://www.youtube.com/watch?v=qiku9Up_DAA"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Optimization"}, "position": {"x": 4600.132565774935, "y": 1924.6248661390598}}, {"data": {"id": "24", "name": "Projection Matrices", "lectures": "Linear Algebra", "description": "", "urls": ["https://www.youtube.com/watch?v=Y_Ac6KiQ1t0", "https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthogonal-projections/v/linear-algebra-projections-onto-subspaces"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 999.0450427995651, "y": 2231.3081205371627}}, {"data": {"id": "25", "name": "Ax=b Least Squares", "lectures": "Linear Algebra", "description": "A method for estimating unknown parameters based on mimizing the squared distance between datapoints and a regression line", "urls": ["http://math.mit.edu/~gs/linearalgebra/ila0403.pdf"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 1796.173566330779, "y": 1976.2362650879782}}, {"data": {"id": "26", "name": "Linear Regression", "lectures": "Probability & Statistics", "description": "A linear approach to estimating the relationship between an depedent and independent variables", "urls": ["https://www.youtube.com/watch?v=ZkjP5RJLQF4", "http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 2117.5860559396174, "y": 2144.8759206175637}}, {"data": {"id": "27", "name": "Orthogonal Matrices", "lectures": "Linear Algebra", "description": "", "urls": ["https://www.youtube.com/watch?v=0MtwqhIwdrI", "https://mathworld.wolfram.com/OrthogonalMatrix.html", "https://www.youtube.com/watch?v=IGBm-gZryVI"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Linear_Algebra"}, "position": {"x": 631.644149814925, "y": 2112.5301496990282}}, {"data": {"id": "28", "name": "Determinants", "lectures": "Linear Algebra", "description": "", "urls": ["https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-determinant-of-2x2-matrix/v/finding-the-determinant-of-a-2x2-matrix", "https://www.khanacademy.org/math/algebra-home/alg-matrices/alg-determinants-and-inverses-of-large-matrices/v/finding-the-determinant-of-a-3x3-matrix-method-2", "https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-18-properties-of-determinants/", "https://www.youtube.com/watch?v=Ip3X9LOh2dk&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab", "https://www.mathsisfun.com/algebra/matrix-inverse-minors-cofactors-adjugate.html", "https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-19-determinant-formulas-and-cofactors/"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 881.0922555208185, "y": 3232.673321253773}}, {"data": {"id": "30", "name": "Eigenvectors & Eigenvalues", "lectures": "Linear Algebra", "description": "A fundamental feature of matrices leading to far deeper understanding of them geometrically.", "urls": ["https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-21-eigenvalues-and-eigenvectors/", "https://www.youtube.com/watch?v=PFDu9oVAE-g"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Linear_Algebra"}, "position": {"x": 1364.3233454532358, "y": 1896.8046866920486}}, {"data": {"id": "31", "name": "Diagonalization", "lectures": "Linear Algebra", "description": "", "urls": ["https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-22-diagonalization-and-powers-of-a/"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 835.1246107226508, "y": 1897.683664292485}}, {"data": {"id": "32", "name": "Transformed distributions", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.khanacademy.org/math/ap-statistics/random-variables-ap/transforming-random-variables/v/impact-of-scaling-and-shifting-random-variables", "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2018/lecture-notes/MIT6_436JF18_lec12.pdf", "https://web.mit.edu/urban_or_book/www/book/chapter3/3.1.html"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3141.037762492871, "y": 3162.380310101896}}, {"data": {"id": "33", "name": "Convolutions", "lectures": "Calculus", "description": "", "urls": ["https://colah.github.io/posts/2014-07-Understanding-Convolutions/", "https://www.khanacademy.org/math/differential-equations/laplace-transform/convolution-integral/v/introduction-to-the-convolution"], "nodetype": "concept", "relative_importance": 1, "parent": "Calculus"}, "position": {"x": 4002.316149255996, "y": 2031.851563105448}}, {"data": {"id": "34", "name": "Central Limit Theorem", "lectures": "Probability & Statistics", "description": "In populations with a mean and standard deviation, if you take a sufficient number of samples, the means of those samples will be normally distributed", "urls": ["https://seeing-theory.brown.edu/probability-distributions/index.html", "https://www.khanacademy.org/math/ap-statistics/sampling-distribution-ap/sampling-distribution-mean/v/central-limit-theorem", "https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 2783.2828326565464, "y": 2052.4790907355223}}, {"data": {"id": "35", "name": "Gaussian Distribution", "lectures": "Probability & Statistics", "description": "A continuous probability distribution to approximate random variables. Also known as the 'normal distribution' or informally as the 'bell curve'", "urls": ["https://www.khanacademy.org/math/statistics-probability/modeling-distributions-of-data/more-on-normal-distributions/v/introduction-to-the-normal-distribution", "https://www.youtube.com/watch?v=iYiOVISWXS4"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Probability_&_Statistics"}, "position": {"x": 2660.8430937722565, "y": 2275.08758080683}}, {"data": {"id": "36", "name": "Covariance", "lectures": "Probability & Statistics", "description": "A measure of the strength of correlation between two or more random variables", "urls": ["https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/covariance-and-the-regression-line", "https://www.youtube.com/watch?v=4EXNedimDMs", "https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/correlation-coefficient-r/v/calculating-correlation-coefficient-rhttps://www.itl.nist.gov/div898/handbook/pmc/section5/pmc541.htm", "https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22#:~:text=%E2%80%9CCovariance%E2%80%9D%20indicates%20the%20direction%20oflinear%20relationship%20between%20two%20variables."], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Probability_&_Statistics"}, "position": {"x": 2442.7723296501626, "y": 2554.189301122863}}, {"data": {"id": "41", "name": "Singular value decomposition", "lectures": "Linear Algebra", "description": "", "urls": ["https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm", "https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d", "https://www.youtube.com/watch?v=mBcLRGuAFUk", "https://www.youtube.com/watch?v=rYz83XPxiZo"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 986.2533792390107, "y": 1683.71771913969}}, {"data": {"id": "42", "name": "Change of basis", "lectures": "Linear Algebra", "description": "", "urls": ["https://www.youtube.com/watch?v=P2LTAUO1TdA", "https://www.youtube.com/watch?v=1j5WnqwMdCk", "https://www.youtube.com/watch?v=0h43aV4aH7I"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Linear_Algebra"}, "position": {"x": 1168.2659002304927, "y": 2087.601173872698}}, {"data": {"id": "45", "name": "Maximum A Posteriori (MAP) estimation", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.youtube.com/watch?v=kkhdIriddSI", "https://machinelearningmastery.com/maximum-a-posteriori-estimation/"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3588.114344291947, "y": 1781.153173846855}}, {"data": {"id": "46", "name": "Maximum Likelihood (ML) estimation", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.youtube.com/watch?v=00krscK7iBA", "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1", "https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3822.0845016005637, "y": 1718.609099414662}}, {"data": {"id": "56", "name": "Generative & Discriminative Models", "lectures": "Machine Learning", "description": "", "urls": ["https://www.youtube.com/watch?v=oTtow2Ui8vg", "https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3#:~:text=In%20General%2C%20A%20Discriminative%20modelactual%20distribution%20of%20each%20class.&text=A%20Discriminative%20model%20%E2%80%8Clearns%20theused%20in%20supervised%20learning%20problems.", "https://stats.stackexchange.com/questions/12421/generative-vs-discriminative"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Machine_Learning"}, "position": {"x": 3374.4056031153764, "y": 1492.441351937193}}, {"data": {"id": "57", "name": "Entropy", "lectures": "Probability & Statistics", "description": "The measure of uncertainty associated with the values taken by a random variable. Can also be described as the quantity of information in a set of random variables", "urls": ["http://colah.github.io/posts/2015-09-Visual-Information/#codes", "https://www.youtube.com/watch?v=YM-uykVfq_E", "https://www.youtube.com/watch?v=8N1BxHgsoOw"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3034.540358845289, "y": 2328.82337070873}}, {"data": {"id": "58", "name": "KL Divergence", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.youtube.com/watch?v=ErfnhcEV1O8", "https://www.youtube.com/watch?v=LJwtEaP2xKA", "https://www.youtube.com/watch?v=xmvxXXZUXdk", "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Probability_&_Statistics"}, "position": {"x": 3049.846191233598, "y": 1730.5852054608317}}, {"data": {"id": "59", "name": "Markov Chains", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/markov_chains"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Probability_&_Statistics"}, "position": {"x": 2073.2942914057385, "y": 2809.1888538362896}}, {"data": {"id": "60", "name": "Classification: Logistic regression & softmax", "lectures": "Machine Learning", "description": "Classification is learning to categorise data into groups based on labels. In more mathematical vernacular, classification is the umbrella term for supervised learning problems with discrete output space.", "urls": ["https://www.youtube.com/watch?v=yIYKR4sgzI8", "https://christophm.github.io/interpretable-ml-book/logistic.html", "https://machinelearningmastery.com/logistic-regression-for-machine-learning/"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 3127.343138082104, "y": 1379.2582324294035}}, {"data": {"id": "61", "name": "Gradient Descent/Ascent", "lectures": "Optimization", "description": "A large family of optimisation algorithms based on a simple observation: to reach the bottom of a valley, try walking downhill.", "urls": ["https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "https://ruder.io/optimizing-gradient-descent/", "https://www.coursera.org/lecture/ml-classification/review-of-gradient-ascent-uwmnm"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Optimization"}, "position": {"x": 4585.214416016208, "y": 1177.5368978420372}}, {"data": {"id": "62", "name": "Regularization & Overfitting", "lectures": "Machine Learning", "description": "Overfitting is a perennial problem in machine learning. If a model is only effective on the dataset it was trained on, and doesn't generalise to the wider world, it isn't useful. Regularization mitigates overfitting by adding a penalty term to disincentivise overfitting.", "urls": ["https://www.youtube.com/watch?v=ndYnUrx8xvs", "https://www.datacamp.com/community/tutorials/towards-preventing-overfitting-regularization", "https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c", "https://towardsdatascience.com/machine-learning-regularization-and-over-fitting-simply-explained-d4dfdc256c9d"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 4196.141910534474, "y": 922.4127611275495}}, {"data": {"id": "63", "name": "Principal Component Analysis", "lectures": "Machine Learning", "description": "", "urls": ["https://builtin.com/data-science/step-step-explanation-principal-component-analysis", "https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 2786.6643691422246, "y": 1193.121434282766}}, {"data": {"id": "64", "name": "Support Vector Machines", "lectures": "Machine Learning", "description": "A class of machine learning models used typically in binary classification. SVM's are trained with supervised learning.", "urls": ["https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47", "https://www.youtube.com/watch?v=efR1C6CvhmE", "http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf", "https://mml-book.github.io/book/mml-book.pdf#page=376"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 3202.0079580297506, "y": 901.9818393031826}}, {"data": {"id": "65", "name": "Perceptrons & Neural Networks", "lectures": "Deep Learning", "description": "The basis of deep learning - that a network of connected neurons have the capacity to learn amazingly powerful and varied representations. This concept starts focused on the simplest case - one neuron - and builds up to many.", "urls": ["https://deepai.org/machine-learning-glossary-and-terms/perceptron#:~:text=A%20Perceptron%20is%20an%20algorithma%20single%2Dlayer%20neural%20network.", "https://www.simplilearn.com/what-is-perceptron-tutorial", "https://www.youtube.com/watch?v=aiDv1NPdXvU", "https://www.youtube.com/watch?v=aircAruvnKk&vl=en", "https://www.youtube.com/watch?v=MfIjxPh6Pys&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=11", "http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Deep_Learning"}, "position": {"x": 3331.96884245093, "y": 624.7943831400106}}, {"data": {"id": "66", "name": "Activation Functions", "lectures": "Deep Learning", "description": "Biological neurons fire when a threshold is reached. Activation functions provide similar non-linearities in artifical neurons.", "urls": ["https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/", "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 3598.89574728623, "y": 538.287932527239}}, {"data": {"id": "67", "name": "Backpropagation", "lectures": "Deep Learning", "description": "The fundamental algorithm that enables training machine learning models with supervised learning, backpropagation updates model parameters to improve its predictions. ", "urls": ["https://www.youtube.com/watch?v=Ilg3gGewQ5U", "https://www.youtube.com/watch?v=tIeHLnjs5U8&vl=en", "https://colah.github.io/posts/2015-08-Backprop/", "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/", "https://brilliant.org/wiki/backpropagation/", "http://neuralnetworksanddeeplearning.com/chap2.html"], "nodetype": "concept", "relative_importance": 1.7146428199482244, "parent": "Deep_Learning"}, "position": {"x": 3974.357540446042, "y": 423.68676331233024}}, {"data": {"id": "74", "name": "Ensemble Methods", "lectures": "Deep Learning", "description": "Instead of relying on the inference of one trained neural network, train many (an ensemble), typically with different architectures, datasets or hyperparameters and average across them", "urls": ["https://www.toptal.com/machine-learning/ensemble-methods-machine-learning#:~:text=Ensemble%20methods%20are%20techniques%20thatthan%20a%20single%20model%20would.&text=These%20models%2C%20when%20used%20asare%20called%20%E2%80%9Dbase%20models%E2%80%9D.", "https://blog.statsbot.co/ensemble-learning-d1dcd548e936"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 3550.4382749797137, "y": -476.82977314975636}}, {"data": {"id": "75", "name": "Dropout", "lectures": "Deep Learning", "description": "A method for regularizing neural networks that approximates training neural networks with many different architectures at once. Reduces the reliance on any individual node.", "urls": ["https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/", "https://jmlr.org/papers/v15/srivastava14a.html", "https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5", "https://www.coursera.org/lecture/convolutional-neural-networks/computer-vision-Ob1nR"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 4573.627426028807, "y": -483.2843373814959}}, {"data": {"id": "77", "name": "Convolutional Neural Networks", "lectures": "Deep Learning", "description": "The fundamental technology underlying all current state-of-the-art approaches to all computer vision problems. Uses a set of learned filters to extract features from images at different scales.", "urls": ["https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53", "https://cs231n.github.io/convolutional-networks/", "https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 4295.464465281176, "y": 278.90727499876874}}, {"data": {"id": "78", "name": "K-means clustering", "lectures": "Machine Learning", "description": "An unsupervised learning algorithm that learns to cluster data points into distinct groups based on their values.", "urls": ["https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1", "https://www.youtube.com/watch?v=4b5d3muPQmA"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 3936.2822249224746, "y": 1491.4222995359735}}, {"data": {"id": "81", "name": "Expectation Maximization", "lectures": "Machine Learning", "description": "", "urls": ["https://machinelearningmastery.com/expectation-maximization-em-algorithm/", "https://www.youtube.com/watch?v=REypj2sy_5U", "https://www.youtube.com/watch?v=rVfZHWTwXSA", "https://www.youtube.com/watch?v=rVfZHWTwXSA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=14"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 3054.133231830501, "y": 1512.643002300509}}, {"data": {"id": "82", "name": "EM Mixture of Gaussians", "lectures": "Machine Learning", "description": "", "urls": ["https://stephens999.github.io/fiveMinuteStats/intro_to_em.html", "https://www.ics.uci.edu/~smyth/courses/cs274/notes/EMnotes.pdf", "http://statweb.stanford.edu/~tibs/stat315a/LECTURES/em.pdf"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 2899.497870724953, "y": 1363.5194326342246}}, {"data": {"id": "83", "name": "Factor Analysis", "lectures": "Machine Learning", "description": "", "urls": ["https://www.youtube.com/watch?v=WV_jcaDBZ2I", "https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/", "https://www.ibm.com/docs/SSLVMB_23.0.0/spss/base/idh_fact.html"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 2476.4251055729264, "y": 1138.1499037289539}}, {"data": {"id": "84", "name": "Probabilistic Graphical Models", "lectures": "Machine Learning", "description": "A large family of probabilistic methods describing the relationships between random variables", "urls": ["https://towardsdatascience.com/introduction-to-probabilistic-graphical-models-b8e0bf459812", "https://ermongroup.github.io/cs228-notes/"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Machine_Learning"}, "position": {"x": 3671.637642476766, "y": 1405.9468059436272}}, {"data": {"id": "85", "name": "Autoencoders", "lectures": "Deep Learning", "description": "", "urls": ["https://www.jeremyjordan.me/autoencoders/", "https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798", "https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726", "https://www.deeplearningbook.org/contents/autoencoders.html"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 4301.821249502484, "y": -475.88209099460073}}, {"data": {"id": "86", "name": "Generative Adversarial Networks", "lectures": "Deep Learning", "description": "", "urls": ["https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/", "https://developers.google.com/machine-learning/gan", "https://openai.com/blog/generative-models/", "https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/", "https://www.youtube.com/watch?v=Gib_kiXgnvA", "https://arxiv.org/abs/1406.2661"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 4011.53649827584, "y": -507.88522955140894}}, {"data": {"id": "87", "name": "Recurrent Neural Networks", "lectures": "Deep Learning", "description": "", "urls": ["https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks", "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/", "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 3325.725150538484, "y": 281.8930951687197}}, {"data": {"id": "88", "name": "LSTM", "lectures": "Deep Learning", "description": "Long Short-Term Memory (or LSTM for short) recurrent neural network neurons take inputs word-embeddings one-by-one.", "urls": ["https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "https://www.youtube.com/watch?v=QciIcRxJvsM", "https://www.youtube.com/watch?v=LfnrRPFhkuY"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 3330.705763858178, "y": -11.67101255569341}}, {"data": {"id": "89", "name": "Gaussian Processes", "lectures": "Machine Learning", "description": "", "urls": ["https://thegradient.pub/gaussian-process-not-quite-for-dummies/", "https://distill.pub/2019/visual-exploration-gaussian-processes", "https://towardsdatascience.com/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 2267.2404781275363, "y": 936.4747341028808}}, {"data": {"id": "92", "name": "Transposes & Symmetry", "lectures": "Linear Algebra", "description": "The transpose of a matrix is the matrix flipped along it's leading diagonal (from top left to bottom right). Square matrices that are identical to their transposes are called symmetric matrices. In other words, symmetric matrices are symmetric when the entries are mirrored across the leading diagonal.", "urls": ["https://stattrek.com/statistics/dictionary.aspx?definition=symmetric_matrix", "https://www.khanacademy.org/math/linear-algebra/matrix-transformations/matrix-transpose/v/linear-algebra-transpose-of-a-matrix", "https://www.youtube.com/watch?v=TZrKrNVhbjI"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 1540.2937249885342, "y": 3167.593125008784}}, {"data": {"id": "93", "name": "Positive Definite Matrices", "lectures": "Linear Algebra", "description": "A key family of matrices with interesting properties and a wide array of use cases.", "urls": ["https://www.youtube.com/watch?v=UCc9q_cAhho", "https://www.math.utah.edu/~zwick/Classes/Fall2012_2270/Lectures/Lecture33_with_Examples.pdf", "https://www.youtube.com/watch?v=ojUQk_GNQbQ"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 1896.3394978076349, "y": 3182.530550594447}}, {"data": {"id": "94", "name": "Markov Matrices", "lectures": "Linear Algebra", "description": "A Markov matrix is a square matrix with all nonnegative entries, and where the sum of the entries down any column is 1", "urls": ["https://www.math.utah.edu/~zwick/Classes/Fall2012_2270/Lectures/Lecture40_with_Examples.pdf", "https://www.youtube.com/watch?v=nnssRe5DewE"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 1896.6437122378363, "y": 2648.922795758767}}, {"data": {"id": "98", "name": "Linear Discriminant Analysis", "lectures": "Machine Learning", "description": "", "urls": ["https://sebastianraschka.com/Articles/2014_python_lda.html", "https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b", "https://www.youtube.com/watch?v=azXCzI57Yfc"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 2646.278812920803, "y": 896.71411151203}}, {"data": {"id": "101", "name": "Markov Decision Processes", "lectures": "Reinforcement Learning", "description": "The fundamental mathematical framework for RL and RL-like problems. A deep understanding of this concept is key to understanding RL.", "urls": ["https://www.youtube.com/watch?v=Jk2V9yA82YU", "https://www.youtube.com/watch?v=9g32v7bK3Co", "https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Reinforcement_Learning"}, "position": {"x": 1826.5947359865538, "y": 1479.087475610667}}, {"data": {"id": "102", "name": "Model-based & Model-free RL", "lectures": "Reinforcement Learning", "description": "A key differentiator that separates RL methods into two families. Model-based directly models how the environment evolves, so can simulate how an action now might affect the future states, while model-free methods do not, so can't simulate ahead.", "urls": ["https://www.youtube.com/watch?v=bFPoHrAoPoQ", "https://neptune.ai/blog/model-based-and-model-free-reinforcement-learning-pytennis-case-study", "https://www.youtube.com/watch?v=nnxHlg-2WgA", "https://www.youtube.com/watch?v=PnHCvfgC_ZA", "https://www.youtube.com/watch?v=_rKzhhDRq_4"], "nodetype": "concept", "relative_importance": 1, "parent": "Reinforcement_Learning"}, "position": {"x": 1514.1736573418111, "y": 1134.066145178693}}, {"data": {"id": "103", "name": "Bellman Equations", "lectures": "Reinforcement Learning", "description": "The foundational equations of RL. These define value functions based on policies and optimal behaviour in Markov Decision Processes.", "urls": ["https://www.youtube.com/watch?v=14BfO5lMiuk", "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "https://medium.com/analytics-vidhya/bellman-equation-and-dynamic-programming-773ce67fc6a7"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Reinforcement_Learning"}, "position": {"x": 1826.2182463992306, "y": 1125.9165167542585}}, {"data": {"id": "104", "name": "Dynamic Programming: Policy & Value Iteration", "lectures": "Reinforcement Learning", "description": "Solve MDPs exactly with these algorithms. A fundamental of RL, key aspects of these approaches are used in almost every RL algorithm.", "urls": ["http://web.mit.edu/15.053/www/AMP-Chapter-11.pdf", "https://www.educative.io/courses/grokking-dynamic-programming-patterns-for-coding-interviews/m2G1pAq0OO0"], "nodetype": "concept", "relative_importance": 1, "parent": "Reinforcement_Learning"}, "position": {"x": 1523.186712358852, "y": 656.8321895198594}}, {"data": {"id": "105", "name": "Monte Carlo Learning", "lectures": "Reinforcement Learning", "description": "RL methods that learn from rollouts of experience in the real environment.", "urls": ["https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf", "https://medium.com/ai%C2%B3-theory-practice-business/reinforcement-learning-part-5-monte-carlo-and-temporal-difference-learning-889053aba07d", "https://towardsdatascience.com/monte-carlo-learning-b83f75233f92", "https://www.youtube.com/watch?v=uiPhlFrwcw8", "https://www.youtube.com/watch?v=mMEFFN1H5Cg"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Reinforcement_Learning"}, "position": {"x": 1828.068617974796, "y": 664.5502877288802}}, {"data": {"id": "106", "name": "Value Function Approximation", "lectures": "Reinforcement Learning", "description": "Apply the Bellman Equations to approximate the value of a state. This is a key component of Q-learning, an algorithm which can achieve the optimal policy.", "urls": ["https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf", "https://web.stanford.edu/class/cs234/slides/lecture5.pdf", "https://www.youtube.com/watch?v=UoPei5o4fps", "https://www.youtube.com/watch?v=buptHUzDKcE", "https://www.youtube.com/watch?v=7Dg6KiI_0eM", "https://www.youtube.com/watch?v=Ijqkc7OLenI"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Reinforcement_Learning"}, "position": {"x": 2023.232411077146, "y": 402.910235313222}}, {"data": {"id": "107", "name": "Actor-Critic", "lectures": "Reinforcement Learning", "description": "A family of RL methods that explicitly train a policy and a value function. These tend to learn faster than both value-based RL methods or policy-based methods.", "urls": ["https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf#page=23", "https://www.youtube.com/watch?v=LawaN3BdI00", "https://www.youtube.com/watch?v=bRfUxQs6xIM", "https://www.youtube.com/watch?v=n6K8FfqQ7ds", "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "http://incompleteideas.net/book/first/ebook/node66.html"], "nodetype": "concept", "relative_importance": 1, "parent": "Reinforcement_Learning"}, "position": {"x": 1729.2407664297577, "y": 122.49932492392105}}, {"data": {"id": "108", "name": "Q-learning", "lectures": "Reinforcement Learning", "description": "As the first RL approach to achieve superhuman play on Atari games, Q-learning has cemented its place as a key RL algorithm.", "urls": ["https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf", "https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c", "https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf", "https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/"], "nodetype": "concept", "relative_importance": 1, "parent": "Reinforcement_Learning"}, "position": {"x": 2021.9776838915477, "y": 136.08242104854952}}, {"data": {"id": "109", "name": "Exploration & Exploitation", "lectures": "Reinforcement Learning", "description": "A classic dichotomy for RL - should I explore (and learn more about the environment) or exploit (and do what I already know works to achieve higher rewards)? How should I balance these two?", "urls": ["https://www.davidsilver.uk/wp-content/uploads/2020/03/XX.pdf", "https://towardsdatascience.com/intro-to-reinforcement-learning-the-explore-exploit-dilemma-463ceb004989", "https://www.manifold.ai/exploration-vs-exploitation-in-reinforcement-learning", "https://www.youtube.com/watch?v=eM6IBYVqXEA", "https://www.youtube.com/watch?v=sGuiWX07sKw"], "nodetype": "concept", "relative_importance": 1, "parent": "Reinforcement_Learning"}, "position": {"x": 2056.942755501688, "y": 1135.7544776991651}}, {"data": {"id": "111", "name": "Markov Random Fields", "lectures": "Machine Learning", "description": "Markov Random Fields (MRFs) have applications in computer vision, graphics and computational biology. They're the undirected counterpart to the directed Bayesian Networks.", "urls": ["https://ermongroup.github.io/cs228-notes/representation/undirected/", "https://www.fmrib.ox.ac.uk/datasets/techrep/tr00yz1/tr00yz1/node4.html", "https://www.youtube.com/watch?v=iBQkZdPHlCs"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 3578.243652854649, "y": 1185.6781114697544}}, {"data": {"id": "112", "name": "Bayesian Networks", "lectures": "Machine Learning", "description": "With uses ranging from prediction, anomaly detection to reasoning and decision making under uncertainty, Bayesian Networks are a highly flexible form of probabilistic graphical model. Bayesian networks are directed graphical models, showing how random variables depend on one another.", "urls": ["https://ermongroup.github.io/cs228-notes/representation/directed/", "https://www.youtube.com/watch?v=tMwfGGkj8DE", "https://www.youtube.com/watch?v=TuGDMj43ehw", "https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Machine_Learning"}, "position": {"x": 3817.3345472746173, "y": 995.6275057985893}}, {"data": {"id": "113", "name": "Variable Elimination", "lectures": "Machine Learning", "description": "The most basic exact inference algorithm for probabilistic graphical models.", "urls": ["https://ermongroup.github.io/cs228-notes/inference/ve/", "https://www.cs.ubc.ca/~kevinlb/teaching/cs322%20-%202006-7/Lectures/lect29.pdf", "https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture4.pdf", "https://www.cs.upc.edu/~larrosa/MEI-CSI-files/BN/2-BN-VE.pdf"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 3737.479234020747, "y": 776.4393172844669}}, {"data": {"id": "114", "name": "Belief Propagation", "lectures": "Machine Learning", "description": "An algorithm for inference in graphical models using message-passing", "urls": ["https://ermongroup.github.io/cs228-notes/inference/jt/", "https://www.ski.org/sites/default/files/publications/bptutorial.pdf", "http://helper.ipam.ucla.edu/publications/gss2013/gss2013_11344.pdf"], "nodetype": "concept", "relative_importance": 1, "parent": "Machine_Learning"}, "position": {"x": 3955.717065396615, "y": 786.3566411119283}}, {"data": {"id": "117", "name": "Multivariate Gaussians", "lectures": "Probability & Statistics", "description": "", "urls": ["http://cs229.stanford.edu/section/gaussians.pdf", "https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/gaussian.pdf", "https://www.youtube.com/watch?v=JjB58InuTqM", "https://www.youtube.com/watch?v=eho8xH3E6mE"], "nodetype": "concept", "relative_importance": 1.5652475842498528, "parent": "Probability_&_Statistics"}, "position": {"x": 2536.843549904935, "y": 1840.596592815984}}, {"data": {"id": "118", "name": "Learning in Sequence Models", "lectures": "Deep Learning", "description": "", "urls": ["https://wiki.pathmind.com/lstm#backpropagation", "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "https://www.youtube.com/watch?v=_i3aqgKVNQI&list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6", "https://www.youtube.com/watch?v=CznICCPa63Q", "https://www.youtube.com/watch?v=oF0Rboc4IJw"], "nodetype": "concept", "relative_importance": 1, "parent": "Deep_Learning"}, "position": {"x": 3331.088102182191, "y": -484.8285870769239}}, {"data": {"id": "119", "name": "Policy Gradient Methods", "lectures": "Reinforcement Learning", "description": "A family of model-free RL methods based on the policy gradient theorem. It's most notable members include PPO, TRPO and REINFORCE.", "urls": ["https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf", "http://www.scholarpedia.org/article/Policy_gradient_methods", "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html", "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "https://www.youtube.com/watch?v=A_2U6Sx67sE", "https://www.youtube.com/watch?v=KHZVXao4qXs"], "nodetype": "concept", "relative_importance": 1, "parent": "Reinforcement_Learning"}, "position": {"x": 1619.6359414919211, "y": 395.33105363568706}}, {"data": {"id": "123", "name": "Estimators", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.youtube.com/watch?v=qvR7sSGphQ4&ab_channel=BenLambert", "https://www.youtube.com/watch?v=UxbY85Cm9SQ&ab_channel=BenLambertBenLambert", "https://warwick.ac.uk/fac/soc/economics/staff/vetroeger/teaching/qrmnew2.pdf", "https://www.statisticshowto.com/estimator/"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Probability_&_Statistics"}, "position": {"x": 3775.402383554325, "y": 2293.8672051555955}}, {"data": {"id": "124", "name": "TD & TD(lambda) learning", "lectures": "Reinforcement Learning", "description": "Temporal Difference (or TD) learning refers to RL methods that learn online while interacting with an environment", "urls": ["https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI", "https://medium.com/@violante.andre/simple-reinforcement-learning-temporal-difference-learning-e883ea0d65b0", "https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html", "https://www.youtube.com/watch?v=L64E_NTZJ_0", "https://www.youtube.com/watch?v=LyCpuLikLyQ", "https://www.youtube.com/watch?v=PnHCvfgC_ZA"], "nodetype": "concept", "relative_importance": 1, "parent": "Reinforcement_Learning"}, "position": {"x": 1822.4320558834718, "y": 402.75057291415135}}, {"data": {"id": "128", "name": "Cross Entropy", "lectures": "Probability & Statistics", "description": "", "urls": ["https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e", "http://neuralnetworksanddeeplearning.com/chap3.html", "https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932", "https://www.youtube.com/watch?v=6ArSys5qHAU"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3049.0513114448954, "y": 1996.887525591172}}, {"data": {"id": "129", "name": "Multivariate PDFs & PMFs", "lectures": "Probability & Statistics", "description": "", "urls": ["https://www.youtube.com/watch?v=PR-A3UAO7_0", "https://www.dam.brown.edu/people/huiwang/classes/am165/Prob_ch5_2007.pdf", "https://pages.ucsd.edu/~rlevy/pmsl_textbook/chapters/pmsl_3.pdf", "https://www.stat.uchicago.edu/~stigler/Stat244/ch3withfigs.pdf", "http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-prob.pdf#page=6"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Probability_&_Statistics"}, "position": {"x": 2718.3667126062455, "y": 3214.3606225429126}}, {"data": {"id": "130", "name": "Stochastic Gradient Descent", "lectures": "Optimization", "description": "A key algorithm used to train many deep learning methods. If it's deep and it learns, chances are it's using SGD or a variant of SGD!", "urls": ["https://www.youtube.com/watch?v=k3AiUhwHQ28", "https://ruder.io/optimizing-gradient-descent/", "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "https://www.youtube.com/watch?v=vMh0zPT0tLI"], "nodetype": "concept", "relative_importance": 1.4, "parent": "Optimization"}, "position": {"x": 4578.505377958141, "y": 837.4930654778354}}, {"data": {"id": "131", "name": "Lagrangian Multipliers", "lectures": "Optimization", "description": "", "urls": ["https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction", "https://tutorial.math.lamar.edu/classes/calciii/lagrangemultipliers.aspx"], "nodetype": "concept", "relative_importance": 1, "parent": "Optimization"}, "position": {"x": 4420.376728274772, "y": 1523.9159935824966}}, {"data": {"id": "132", "name": "Partial derivatives", "lectures": "Calculus", "description": "", "urls": ["https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction", "https://mml-book.github.io/book/mml-book.pdf#page=152", "https://www.youtube.com/watch?v=JAf_aSIJryg", "https://www.mathsisfun.com/calculus/derivatives-partial.html"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Calculus"}, "position": {"x": 4381.980506576692, "y": 2318.3877039184254}}, {"data": {"id": "133", "name": "Product Rule of Probability", "lectures": "Probability & Statistics", "description": "Along with the sum rule, the product rule is a fundamental rule of probability, from which Bayes Rule is defined.", "urls": ["https://mml-book.github.io/book/mml-book.pdf#page=190", "https://brilliant.org/wiki/probability-rule-of-product/", "https://www.youtube.com/watch?v=1O2EBfQ-MgU"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3344.6284841261945, "y": 2368.7174988769275}}, {"data": {"id": "134", "name": "Intro to Probability", "lectures": "Probability & Statistics", "description": "What is random? Something you can't predict the outcome of? If something is random, can we say anything mathematical about things that are random? This concept answers these questions and, more importantly, gives you a basic intuition to probability. ", "urls": ["https://seeing-theory.brown.edu/basic-probability/index.html", "https://www.youtube.com/watch?v=SkidyDQuupA", "https://www.youtube.com/watch?v=uzkc-qNVoOk", "https://www.youtube.com/watch?v=KzfWUEJjG18", "https://www.youtube.com/watch?v=1neg5RigPOU"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 2926.248458592724, "y": 4215.3630736125715}}, {"data": {"id": "137", "name": "Properties of symmetric matrices", "lectures": "Linear Algebra", "description": "Symmetric matrices have many interesting properties. This brings together a lot of linear algebra into one cohesive whole, so important if you want a deep understanding of linear algebra!", "urls": ["https://www.youtube.com/watch?v=UCc9q_cAhho", "https://people.math.carleton.ca/~kcheung/math/notes/MATH1107/wk10/10_symmetric_matrices.html", "https://people.revoledu.com/kardi/tutorial/LinearAlgebra/SymmetricMatrix.html"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 1537.728305657753, "y": 1720.4605447759552}}, {"data": {"id": "138", "name": "Intro to Sets", "lectures": "Probability & Statistics", "description": "Currently the only entry for discrete mathematics on the map, sets are a fundamental concept to understand to have a deep understanding of probablility, discrete maths and it even plays a role in understanding subspaces in linear algebra.", "urls": ["https://seeing-theory.brown.edu/compound-probability/index.html#section1", "https://www.youtube.com/watch?v=rRk0d6P4oUI", "https://plato.stanford.edu/entries/set-theory/basic-set-theory.html", "https://math.libretexts.org/Bookshelves/Analysis/Introduction_to_Mathematical_Analysis_I_(Lafferriere_Lafferriere_and_Nguyen)/01%3A_Tools_for_Analysis/1.01%3A_Basic_Concepts_of_Set_Theory"], "nodetype": "concept", "relative_importance": 1, "parent": "Probability_&_Statistics"}, "position": {"x": 3144.830897442196, "y": 4211.326517318944}}, {"data": {"id": "140", "name": "Intro to Vectors", "lectures": "Linear Algebra", "description": "What's the difference between velocity and speed? What are vectors and scalars? Well, here you'll find out and should learn that vectors are really fundamental objects in mathematics and have many useful applications in computers: from machine learning to 2D and 3D graphics.", "urls": ["https://www.youtube.com/watch?v=fNk_zzaMoSs", "https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra?modal=1", "https://mathinsight.org/vector_introduction", "https://www.youtube.com/watch?v=_YkIivLaVJs"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Linear_Algebra"}, "position": {"x": 1306.509857862238, "y": 3956.113036650485}}, {"data": {"id": "142", "name": "Vector addition & multiplying by scalars", "lectures": "Linear Algebra", "description": "What does it mean to add 2 vectors? Can we... multiply vectors by numbers? Yes we can! This adds a key new level of understanding of what a vector is and how they can be used to your toolbox.", "urls": ["https://www.youtube.com/watch?v=8QihetGj3pg", "https://www.youtube.com/watch?v=KBSCMTYaH1s", "https://youtu.be/ZN7YaSbY3-w"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 1056.923352346099, "y": 3751.982341321946}}, {"data": {"id": "143", "name": "Dot product", "lectures": "Linear Algebra", "description": "What does it mean to multiply two vectors? Well with vectors there are two types of product: dot products and cross products. Dot products are more common and basic - let's focus on these here!", "urls": ["https://www.youtube.com/watch?v=0iNrGpwZwog", "https://www.youtube.com/watch?v=WNuIhXo39_k", "https://www.youtube.com/watch?v=KDHuWxy53uM", "https://www.mathsisfun.com/algebra/vectors-dot-product.html"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 1765.3505749065416, "y": 3669.4667766401194}}, {"data": {"id": "144", "name": "Span & linear independence", "lectures": "Linear Algebra", "description": "We now know that multiplying a scalar by a vector always produces a vector in the same direction as the original vector. So this leads to us being able to reach any point along this line. Can we generalise this concept? What about sums of different vectors? What is required to reach any point in a plane?", "urls": ["https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-independence/v/linear-algebra-introduction-to-linear-independence", "https://www.youtube.com/watch?v=k7RM-ot2NWY"], "nodetype": "concept", "relative_importance": 1.212435565298214, "parent": "Linear_Algebra"}, "position": {"x": 581.0983896019193, "y": 3334.0657397787477}}, {"data": {"id": "145", "name": "Cross product", "lectures": "Linear Algebra", "description": "The second, more challenging, type of vector multiplication, the cross product has important geometric properties.", "urls": ["https://www.youtube.com/watch?v=Zy7RfbURZa4", "https://www.youtube.com/watch?v=pJzmiywagfY", "https://www.youtube.com/watch?v=eu6i7WJeinw", "https://www.youtube.com/watch?v=gPnWm-IXoAY", "https://www.youtube.com/watch?v=pWbOisq1MJU"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 365.6204597238164, "y": 3139.4453161043384}}, {"data": {"id": "147", "name": "Matrix-vector multiplication", "lectures": "Linear Algebra", "description": "We know that vectors can represent magnitude and direction in a 2D or 3D (or any D!) space. This can be thought of geometrically. But what does it mean to multiply a vector by a matrix geometrically?", "urls": ["https://www.youtube.com/watch?v=7Mo4S2wyMg4", "https://www.youtube.com/watch?v=Awcj447pYuk", "https://www.youtube.com/watch?v=kYB8IZa5AuE", "https://www.youtube.com/watch?v=4PCktDZJH8E", "https://www.youtube.com/watch?v=v8VSDg_WQlA", "https://www.youtube.com/watch?v=cFIRXQBfgg0"], "nodetype": "concept", "relative_importance": 1, "parent": "Linear_Algebra"}, "position": {"x": 1297.9699967991955, "y": 3254.9360614860975}}, {"data": {"id": "148", "name": "Limits & continuity", "lectures": "Calculus", "description": "How many numbers are there between 0 and 1? How many numbers are there between 0.9 and 1? How about 0.99 and 1? ", "urls": ["https://www.youtube.com/watch?v=kfF40MiS7zA", "https://www.youtube.com/watch?v=riXcZT2ICjA", "https://www.youtube.com/watch?v=kdEQGfeC0SE"], "nodetype": "concept", "relative_importance": 1, "parent": "Calculus"}, "position": {"x": 4328.221903161308, "y": 4203.148548070533}}], "edges": [{"data": {"id": "147_2", "source": "147", "target": "2"}}, {"data": {"id": "28_3", "source": "28", "target": "3"}}, {"data": {"id": "2_3", "source": "2", "target": "3"}}, {"data": {"id": "3_4", "source": "3", "target": "4"}}, {"data": {"id": "144_5", "source": "144", "target": "5"}}, {"data": {"id": "3_5", "source": "3", "target": "5"}}, {"data": {"id": "27_7", "source": "27", "target": "7"}}, {"data": {"id": "10_9", "source": "10", "target": "9"}}, {"data": {"id": "134_10", "source": "134", "target": "10"}}, {"data": {"id": "138_10", "source": "138", "target": "10"}}, {"data": {"id": "9_11", "source": "9", "target": "11"}}, {"data": {"id": "11_12", "source": "11", "target": "12"}}, {"data": {"id": "20_12", "source": "20", "target": "12"}}, {"data": {"id": "11_13", "source": "11", "target": "13"}}, {"data": {"id": "129_14", "source": "129", "target": "14"}}, {"data": {"id": "11_15", "source": "11", "target": "15"}}, {"data": {"id": "133_16", "source": "133", "target": "16"}}, {"data": {"id": "148_17", "source": "148", "target": "17"}}, {"data": {"id": "17_18", "source": "17", "target": "18"}}, {"data": {"id": "17_19", "source": "17", "target": "19"}}, {"data": {"id": "17_20", "source": "17", "target": "20"}}, {"data": {"id": "23_22", "source": "23", "target": "22"}}, {"data": {"id": "17_23", "source": "17", "target": "23"}}, {"data": {"id": "5_24", "source": "5", "target": "24"}}, {"data": {"id": "4_25", "source": "4", "target": "25"}}, {"data": {"id": "36_26", "source": "36", "target": "26"}}, {"data": {"id": "5_27", "source": "5", "target": "27"}}, {"data": {"id": "1_28", "source": "1", "target": "28"}}, {"data": {"id": "4_30", "source": "4", "target": "30"}}, {"data": {"id": "42_30", "source": "42", "target": "30"}}, {"data": {"id": "5_31", "source": "5", "target": "31"}}, {"data": {"id": "11_32", "source": "11", "target": "32"}}, {"data": {"id": "20_33", "source": "20", "target": "33"}}, {"data": {"id": "35_34", "source": "35", "target": "34"}}, {"data": {"id": "15_35", "source": "15", "target": "35"}}, {"data": {"id": "15_36", "source": "15", "target": "36"}}, {"data": {"id": "129_36", "source": "129", "target": "36"}}, {"data": {"id": "30_41", "source": "30", "target": "41"}}, {"data": {"id": "31_41", "source": "31", "target": "41"}}, {"data": {"id": "24_42", "source": "24", "target": "42"}}, {"data": {"id": "16_45", "source": "16", "target": "45"}}, {"data": {"id": "123_45", "source": "123", "target": "45"}}, {"data": {"id": "123_46", "source": "123", "target": "46"}}, {"data": {"id": "16_56", "source": "16", "target": "56"}}, {"data": {"id": "11_57", "source": "11", "target": "57"}}, {"data": {"id": "128_58", "source": "128", "target": "58"}}, {"data": {"id": "11_59", "source": "11", "target": "59"}}, {"data": {"id": "56_60", "source": "56", "target": "60"}}, {"data": {"id": "19_61", "source": "19", "target": "61"}}, {"data": {"id": "22_61", "source": "22", "target": "61"}}, {"data": {"id": "132_61", "source": "132", "target": "61"}}, {"data": {"id": "61_62", "source": "61", "target": "62"}}, {"data": {"id": "117_63", "source": "117", "target": "63"}}, {"data": {"id": "30_63", "source": "30", "target": "63"}}, {"data": {"id": "60_64", "source": "60", "target": "64"}}, {"data": {"id": "131_64", "source": "131", "target": "64"}}, {"data": {"id": "42_64", "source": "42", "target": "64"}}, {"data": {"id": "27_64", "source": "27", "target": "64"}}, {"data": {"id": "2_65", "source": "2", "target": "65"}}, {"data": {"id": "65_66", "source": "65", "target": "66"}}, {"data": {"id": "61_67", "source": "61", "target": "67"}}, {"data": {"id": "66_67", "source": "66", "target": "67"}}, {"data": {"id": "67_74", "source": "67", "target": "74"}}, {"data": {"id": "62_75", "source": "62", "target": "75"}}, {"data": {"id": "67_75", "source": "67", "target": "75"}}, {"data": {"id": "130_75", "source": "130", "target": "75"}}, {"data": {"id": "33_77", "source": "33", "target": "77"}}, {"data": {"id": "67_77", "source": "67", "target": "77"}}, {"data": {"id": "130_77", "source": "130", "target": "77"}}, {"data": {"id": "23_78", "source": "23", "target": "78"}}, {"data": {"id": "58_81", "source": "58", "target": "81"}}, {"data": {"id": "117_82", "source": "117", "target": "82"}}, {"data": {"id": "81_82", "source": "81", "target": "82"}}, {"data": {"id": "117_83", "source": "117", "target": "83"}}, {"data": {"id": "56_84", "source": "56", "target": "84"}}, {"data": {"id": "45_84", "source": "45", "target": "84"}}, {"data": {"id": "46_84", "source": "46", "target": "84"}}, {"data": {"id": "130_85", "source": "130", "target": "85"}}, {"data": {"id": "67_85", "source": "67", "target": "85"}}, {"data": {"id": "58_85", "source": "58", "target": "85"}}, {"data": {"id": "77_86", "source": "77", "target": "86"}}, {"data": {"id": "56_86", "source": "56", "target": "86"}}, {"data": {"id": "65_87", "source": "65", "target": "87"}}, {"data": {"id": "87_88", "source": "87", "target": "88"}}, {"data": {"id": "117_89", "source": "117", "target": "89"}}, {"data": {"id": "1_92", "source": "1", "target": "92"}}, {"data": {"id": "1_93", "source": "1", "target": "93"}}, {"data": {"id": "2_94", "source": "2", "target": "94"}}, {"data": {"id": "59_94", "source": "59", "target": "94"}}, {"data": {"id": "93_94", "source": "93", "target": "94"}}, {"data": {"id": "63_98", "source": "63", "target": "98"}}, {"data": {"id": "83_98", "source": "83", "target": "98"}}, {"data": {"id": "59_101", "source": "59", "target": "101"}}, {"data": {"id": "101_102", "source": "101", "target": "102"}}, {"data": {"id": "101_103", "source": "101", "target": "103"}}, {"data": {"id": "102_104", "source": "102", "target": "104"}}, {"data": {"id": "103_104", "source": "103", "target": "104"}}, {"data": {"id": "103_105", "source": "103", "target": "105"}}, {"data": {"id": "105_106", "source": "105", "target": "106"}}, {"data": {"id": "106_107", "source": "106", "target": "107"}}, {"data": {"id": "119_107", "source": "119", "target": "107"}}, {"data": {"id": "106_108", "source": "106", "target": "108"}}, {"data": {"id": "101_109", "source": "101", "target": "109"}}, {"data": {"id": "84_111", "source": "84", "target": "111"}}, {"data": {"id": "84_112", "source": "84", "target": "112"}}, {"data": {"id": "112_113", "source": "112", "target": "113"}}, {"data": {"id": "112_114", "source": "112", "target": "114"}}, {"data": {"id": "3_117", "source": "3", "target": "117"}}, {"data": {"id": "35_117", "source": "35", "target": "117"}}, {"data": {"id": "36_117", "source": "36", "target": "117"}}, {"data": {"id": "67_118", "source": "67", "target": "118"}}, {"data": {"id": "88_118", "source": "88", "target": "118"}}, {"data": {"id": "105_119", "source": "105", "target": "119"}}, {"data": {"id": "13_123", "source": "13", "target": "123"}}, {"data": {"id": "15_123", "source": "15", "target": "123"}}, {"data": {"id": "105_124", "source": "105", "target": "124"}}, {"data": {"id": "57_128", "source": "57", "target": "128"}}, {"data": {"id": "11_129", "source": "11", "target": "129"}}, {"data": {"id": "61_130", "source": "61", "target": "130"}}, {"data": {"id": "23_131", "source": "23", "target": "131"}}, {"data": {"id": "132_131", "source": "132", "target": "131"}}, {"data": {"id": "17_132", "source": "17", "target": "132"}}, {"data": {"id": "14_133", "source": "14", "target": "133"}}, {"data": {"id": "92_137", "source": "92", "target": "137"}}, {"data": {"id": "30_137", "source": "30", "target": "137"}}, {"data": {"id": "140_142", "source": "140", "target": "142"}}, {"data": {"id": "140_143", "source": "140", "target": "143"}}, {"data": {"id": "142_144", "source": "142", "target": "144"}}, {"data": {"id": "144_145", "source": "144", "target": "145"}}, {"data": {"id": "143_147", "source": "143", "target": "147"}}, {"data": {"id": "1_147", "source": "1", "target": "147"}}]}
